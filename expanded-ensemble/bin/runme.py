__author__ = "Vivek Balasubramanian <vivek.balasubramanian@rutgers.edu>"
__copyright__ = "Copyright 2016, http://radical.rutgers.edu"
__license__ = "MIT"

from radical.entk import Pipeline, Stage, Task, AppManager
import argparse
import os
import glob


# User settings
ENSEMBLE_SIZE = 16    # Number of ensemble members / pipelines
PIPELINE_SIZE = 4     # Number of stages in each pipeline, currently 4
TOTAL_ITERS = 5       # Number of iterations to run current trial
SEED = 1            # Seed for stage 1


# The following are helpful if we divide our entire experiment of N iterations
# over M trials due to (say) walltime limitations
DONE = 0              # Number of iterations already DONE
ITER = [(DONE + 1) for x in range(1, ENSEMBLE_SIZE + 1)]      # Iteration number to start with
DATA_LOC = ''       # Location where data from iterations 0 to DONE-1 is located


# Handling failed tasks
FAILED = [False for x in range(1, ENSEMBLE_SIZE + 1)]


def get_pipeline(instance):

    p = Pipeline()

    # Stage 1 simply processes the parameter files to get started.
    # Runs only for the first iteration

    global SEED, ITER

    s1 = Stage()

    t = Task()
    t.pre_exec = ['module load python']
    t.executable = 'python'
    t.arguments = ['analysis_1.py',
                   '--template', 'PLCpep7_template.mdp',
                   '--newname', 'PLCpep7_run.mdp',
                   '--wldelta', '100',
                   '--equilibrated', 'False',
                   '--lambda_state', '0',
                   '--seed', '%s' % SEED]
    t.cores = 1
    t.copy_input_data = ['$SHARED/PLCpep7_template.mdp', '$SHARED/analysis_1.py']
    s1.add_tasks(t)
    p.add_stages(s1)

    # Stage 2 is the gromacs preprocessing stage. In the first iteration, uses the output of stage 1. Otherwise
    # operates over the .gro file from stage 3 of the previous iteration and .mdp file from stage 4 of the previous
    # of iteration

    s2 = Stage()

    t = Task()
    t.executable = 'gmx grompp'
    t.pre_exec = ['source /home/vivek91/modules/gromacs_serial/bin/GMXRC.bash']
    t.arguments = ['-f', 'PLCpep7_run.mdp',
                   '-c', 'PLCpep7.gro',
                   '-p', 'PLCpep7.top',
                   '-n', 'PLCpep7.ndx',
                   '-o', 'PLCpep7.tpr',
                   '-maxwarn', '10']
    t.cores = 1
    t.copy_input_data = [
        '$SHARED/PLCpep7.ndx',
        '$SHARED/PLCpep7.top'
    ]

    global ITER

    print 'iter: {0}, stage: 2 . instance: {1}'.format(ITER[instance - 1], instance)

    if (ITER[instance - 1] == DONE + 1):

        if DONE == 0:
            k2.copy_input_data += ['$ITER_1_STAGE_1_TASK_{0}/PLCpep7_run.mdp'.format(instance), '$SHARED/PLCpep7.gro']
        else:
            k2.copy_input_data += ['{2}/PLCpep7_run_iter{1}_pipe{0}.mdp > PLCpep7_run.mdp'.format(instance, DONE, DATA_LOC),
                                   '{2}/PLCpep7_iter{1}_pipe{0}.gro > PLCpep7.gro'.format(instance, DONE, DATA_LOC)]
    else:
        k2.copy_input_data += ['$ITER_{1}_STAGE_4_TASK_{0}/PLCpep7_run.mdp'.format(instance, ITER[instance - 1] - 1 - DONE),
                               '$ITER_{1}_STAGE_3_TASK_{0}/PLCpep7.gro'.format(instance, ITER[instance - 1] - 1 - DONE)]

    s2.add_tasks(t)
    p.add_stages(s2)

    # Stage 3 is the compute intensive gromacs mdrun stage. It operates over the .tpr file generated by stage 2.
    # Stages its output (*.xvg, *.log) to shared location on remote -- 'staging_area' under the pilot folder. The
    # same output is also downloaded to the local machine to keep a backup.

    s3 = Stage()

    t = Task()
    t.executable = 'gmx mdrun'
    t.pre_exec = ['source /home/vivek91/modules/gromacs_serial/bin/GMXRC.bash']
    t.arguments = ['-nt', '20',
                   '-deffnm', 'PLCpep7',
                   '-dhdl', 'PLCpep7_dhdl.xvg']
    t.cores = 20
    t.copy_input_data = ['$STAGE_2_TASK_{0}/PLCpep7.tpr'.format(instance)]

    t.copy_output_data = ['PLCpep7_dhdl.xvg > $SHARED/PLCpep7_run{1}_gen{0}_dhdl.xvg'.format(ITER[instance - 1], instance),
                          'PLCpep7_pullf.xvg > $SHARED/PLCpep7_run{1}_gen{0}_pullf.xvg'.format(
                              ITER[instance - 1], instance),
                          'PLCpep7_pullx.xvg > $SHARED/PLCpep7_run{1}_gen{0}_pullx.xvg'.format(
        ITER[instance - 1], instance),
        'PLCpep7.log > $SHARED/PLCpep7_run{1}_gen{0}.log'.format(ITER[instance - 1], instance)]

    t.download_output_data = ['PLCpep7.xtc > PLCpep7_run{1}_gen{0}.xtc'.format(ITER[instance - 1], instance),
                              'PLCpep7.log > PLCpep7_run{1}_gen{0}.log'.format(ITER[instance - 1], instance),
                              'PLCpep7_dhdl.xvg > PLCpep7_run{1}_gen{0}_dhdl.xvg'.format(ITER[instance - 1], instance),
                              'PLCpep7_pullf.xvg > PLCpep7_run{1}_gen{0}_pullf.xvg'.format(
        ITER[instance - 1], instance),
        'PLCpep7_pullx.xvg > PLCpep7_run{1}_gen{0}_pullx.xvg'.format(
        ITER[instance - 1], instance),
        'PLCpep7.gro > PLCpep7_run{1}_gen{0}.gro'.format(ITER[instance - 1], instance)]

    s3.add_tasks(t)
    p.add_stages(s3)

    # Stage 4 executes the alchemical analysis script and prepares the .mdp file for the next iteration.
    # It currently operates on all data (*.xvg, *.log) that is available at that moment in './data'.
    # './data' maps to the 'staging_area' that was referred to in stage 3. Downloads the results, output, error and
    # the new mdp file to the local machine to keep a backup

    s4 = Stage()

    t = Task()
    t.pre_exec = ['module load python',
                  'export PYTHONPATH=/home/vivek91/modules/alchemical-analysis/alchemical_analysis:$PYTHONPATH',
                  'export PYTHONPATH=/home/vivek91/modules/alchemical-analysis:$PYTHONPATH',
                  'export PYTHONPATH=/home/vivek91/.local/lib/python2.7/site-packages:$PYTHONPATH',
                  'ln -s ../staging_area data']
    t.executable = 'python'
    t.arguments = ['analysis_2.py',
                   '--template', 'PLCpep7_template.mdp',
                   '--newname', 'PLCpep7_run.mdp',
                   '--dir', './data',
                   '--run', instance,
                   '--gen', ITER[instance-1]]

    t.cores = 1

    t.link_input_data = ['$SHARED/analysis_2.py',
                         '$SHARED/alchemical_analysis.py',
                         '$SHARED/PLCpep7_template.mdp',
                         ]

    if ITER[instance - 1] > DONE + 1:
        t.link_input_data += ['$ITER_{1}_STAGE_4_TASK_{0}/analyze_1/results.txt > results_bak.txt'.format(instance, ITER[
                                                                                                          instance - 1] - 1 - DONE)]

    t.download_output_data = ['analyze_1/results.txt > results_run{1}_gen{0}.txt'.format(ITER[instance - 1], instance),
                              'STDOUT > stdout_run{1}_gen{0}'.format(ITER[instance - 1], instance),
                              'STDERR > stderr_run{1}_gen{0}'.format(ITER[instance - 1], instance),
                              'PLCpep7_run.mdp > PLCpep7_run{1}_gen{0}.mdp'.format(ITER[instance - 1], instance)
                              ]

    s4.add_tasks(t)
    p.add_stages(s4)

    def branch_4(self, instance):

        #convergence = self.get_output(stage=5,instance=instance)
        # print 'Convergence of pipeline: ', convergence

        global ITER
        global TOTAL_ITERS

        if ITER[instance - 1] != TOTAL_ITERS:
            ITER[instance - 1] += 1
            self.set_next_stage(stage=2)
        else:
            pass

        # if convergence > 1:
            # self.set_next_stage(stage=2)
        # else:
        #    pass


if __name__ == '__main__':

    # Create pattern object with desired ensemble size, pipeline size
    pipe = Test(ensemble_size=ENSEMBLE_SIZE, pipeline_size=PIPELINE_SIZE)

    # Create an application manager
    app = AppManager(name='Expanded_Ensemble', on_error='continue')

    # Add workload to the application manager
    app.add_workload(pipe)

    # Download analysis file from MobleyLab repo
    os.system('curl -O https://raw.githubusercontent.com/MobleyLab/alchemical-analysis/master/alchemical_analysis/alchemical_analysis.py')

    # Parsing user cmd to get resource
    parser = argparse.ArgumentParser()
    parser.add_argument('--resource', help='target resource label')
    args = parser.parse_args()

    if args.resource != None:
        resource = args.resource
    else:
        resource = 'local.localhost'

    # Resource description to switch between resources
    res_dict = {

        'xsede.stampede': {'cores': '64',
                           'project': 'TG-MCB090174',
                           'queue': 'development',
                           'path': '/home/vivek91/repos/expanded-ensemble'
                           },
        'xsede.comet': {'cores': '288',
                        'project': 'unc101',
                        'queue': 'compute',
                        'path': '/home/vivek/expanded-ensemble'
                        },
        'xsede.supermic': {'cores': '340',
                           'project': 'TG-MCB090174',
                           'queue': 'workq'
                           }
    }

    # Create a resource handle for target machine
    res = ResourceHandle(resource=resource,
                         cores=res_dict[resource]['cores'],
                         # username='',
                         project=res_dict[resource]['project'],
                         queue=res_dict[resource]['queue'],
                         walltime=60,
                         database_url='mongodb://rp:rp@ds139430.mlab.com:39430/ee_exp_4c',
                         access_schema='gsissh'
                         )

    # Data common to multiple tasks -- transferred only once to common staging area
    res.shared_data = ['./PLCpep7.gro', './PLCpep7.ndx', './PLCpep7.top',
                       './PLCpep7_template.mdp', './analysis_1.py',
                       './analysis_2.py', './determine_convergence.py',
                       './alchemical_analysis.py']

    try:
        # Submit request for resources + wait till job becomes Active
        res.allocate(wait=True)

        # Run the given workload
        res.run(app)

    except Exception, ex:
        print 'Error, ', ex

    finally:
        # Deallocate the resource
        res.deallocate()
